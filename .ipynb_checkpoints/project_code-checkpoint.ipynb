{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 427,
     "status": "ok",
     "timestamp": 1686860533043,
     "user": {
      "displayName": "Fatih Erdoğan",
      "userId": "07677762697162902920"
     },
     "user_tz": -180
    },
    "id": "Apl9duq1NBTH",
    "outputId": "39486446-0923-4c29-e146-e0e8a08694c4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Code_BabyLM\n"
     ]
    }
   ],
   "source": [
    "cd /content/drive/MyDrive/Code_BabyLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 562,
     "status": "ok",
     "timestamp": 1686860552537,
     "user": {
      "displayName": "Fatih Erdoğan",
      "userId": "07677762697162902920"
     },
     "user_tz": -180
    },
    "id": "4y83Ow0mGXRK"
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1686860552537,
     "user": {
      "displayName": "Fatih Erdoğan",
      "userId": "07677762697162902920"
     },
     "user_tz": -180
    },
    "id": "kfCdhpdIAD6y"
   },
   "outputs": [],
   "source": [
    "global tokenizer_p1, tokenizer_p2, tokenizer_p3\n",
    "tokenizer_p1 = None\n",
    "tokenizer_p2 = None\n",
    "tokenizer_p3 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 456,
     "status": "ok",
     "timestamp": 1686861210230,
     "user": {
      "displayName": "Fatih Erdoğan",
      "userId": "07677762697162902920"
     },
     "user_tz": -180
    },
    "id": "t0tPmn4jGRUl"
   },
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from transformers import RobertaTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "def get_tokenizer(phase):\n",
    "\n",
    "  ########### Paths for all the training files that will be used during training ###########\n",
    "  ##########################################################################################\n",
    "  path = \"babylm_data/babylm_10M/\"\n",
    "  train_files = [path+\"1-aochildes.train\", path+\"2-qed.train\", path+\"3-open_subtitles.train\",\n",
    "           path+\"4-switchboard.train\", path+\"5-cbt.train\", path+\"6-children_stories.train\",\n",
    "           path+\"7-gutenberg.train\", path+\"8-simple_wikipedia.train\",\n",
    "           path+\"9-wikipedia.train\", path+\"99-bnc_spoken.train\"]\n",
    "\n",
    "  global tokenizer_p1, tokenizer_p2, tokenizer_p3\n",
    "\n",
    "  voc_size = None\n",
    "\n",
    "  if phase==1:\n",
    "    if tokenizer_p1 is not None:\n",
    "      return tokenizer_p1\n",
    "    voc_size = 5334\n",
    "  elif phase==2:\n",
    "    if tokenizer_p2 is not None:\n",
    "      return tokenizer_p2\n",
    "    voc_size = 15334\n",
    "  elif phase==3:\n",
    "    if tokenizer_p3 is not None:\n",
    "      return tokenizer_p3\n",
    "    voc_size = 30334\n",
    "\n",
    "  tokenizer = ByteLevelBPETokenizer()\n",
    "  tokenizer.train(\n",
    "    files=train_files,\n",
    "    vocab_size=voc_size,\n",
    "    min_frequency=2,\n",
    "    special_tokens=[\n",
    "        \"<s>\",\n",
    "        \"<pad>\",\n",
    "        \"</s>\",\n",
    "        \"<unk>\",\n",
    "        \"<mask>\",\n",
    "    ]\n",
    "  )\n",
    "\n",
    "  ##### Path to save the tokenizer that will be used to create a RobertaTokenizer #####\n",
    "  #####################################################################################\n",
    "  tokenizer.save_model(f\"tokenizers/customTokenizers/forP{str(phase)}\")\n",
    "\n",
    "  ########## The switch of tokenizer due to the compatibility issues ##########\n",
    "  #############################################################################\n",
    "  tokenizer = RobertaTokenizer.from_pretrained(f\"tokenizers/customTokenizers/forP{str(phase)}\", max_len=512)\n",
    "  tokenizer.save_pretrained(f\"tokenizers/finalTokenizers/forP{str(phase)}\")\n",
    "\n",
    "  if phase==1:\n",
    "    tokenizer_p1 = tokenizer\n",
    "  elif phase==2:\n",
    "    tokenizer_p2 = tokenizer\n",
    "  elif phase==3:\n",
    "    tokenizer_p3 = tokenizer\n",
    "  else:\n",
    "    assert False, \"Phase not provided\"\n",
    "\n",
    "\n",
    "  return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 384,
     "status": "ok",
     "timestamp": 1686860560194,
     "user": {
      "displayName": "Fatih Erdoğan",
      "userId": "07677762697162902920"
     },
     "user_tz": -180
    },
    "id": "7Cl0SMQKGLZp"
   },
   "outputs": [],
   "source": [
    "def init_model_and_tokenizer(phase):\n",
    "  tokenizer = None\n",
    "  if phase == 1:\n",
    "    tokenizer = get_tokenizer(1)\n",
    "    config = RobertaConfig(hidden_size=192, intermediate_size=768, num_of_attention_heads=3, num_hidden_layers=3, vocab_size=len(tokenizer.get_vocab()))\n",
    "  elif phase == 2:\n",
    "    tokenizer = get_tokenizer(2)\n",
    "    config = RobertaConfig(hidden_size=384, intermediate_size=1536, num_of_attention_heads=6, num_hidden_layers=6, vocab_size=len(tokenizer.get_vocab()))\n",
    "  elif phase == 3:\n",
    "    tokenizer = get_tokenizer(3)\n",
    "    config = RobertaConfig(hidden_size=768, intermediate_size=3072, num_of_attention_heads=12, num_hidden_layers=12, vocab_size=len(tokenizer.get_vocab()))\n",
    "  else:\n",
    "    assert False, \"Phase not provided\"\n",
    "\n",
    "  return (AutoModelForMaskedLM.from_config(config), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 9224,
     "status": "ok",
     "timestamp": 1686860569826,
     "user": {
      "displayName": "Fatih Erdoğan",
      "userId": "07677762697162902920"
     },
     "user_tz": -180
    },
    "id": "ehstlrLpigcD"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1686860569827,
     "user": {
      "displayName": "Fatih Erdoğan",
      "userId": "07677762697162902920"
     },
     "user_tz": -180
    },
    "id": "_MB8hs3ob4Rm"
   },
   "outputs": [],
   "source": [
    "def merge_type1(t_from, t_to):\n",
    "  assert t_from.dim() == t_to.dim(), \"Dimensions, from-to don't hold\"\n",
    "  assert t_from.dim() == 1, f\"Expected 1D tensor, received {t_from.dim()}\"\n",
    "  to_ret = torch.cat((t_from, t_to[t_from.size()[-1]:]))\n",
    "  return to_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1686860569827,
     "user": {
      "displayName": "Fatih Erdoğan",
      "userId": "07677762697162902920"
     },
     "user_tz": -180
    },
    "id": "ZAjecMepejs2"
   },
   "outputs": [],
   "source": [
    "def merge_type2(t_from, t_to):\n",
    "  assert t_from.dim() == t_to.dim(), \"Dimensions, from-to don't hold\"\n",
    "  assert t_from.dim() == 2, f\"Expected 1D tensor, received {t_from.dim()}\"\n",
    "  to_ret = torch.cat((t_from, t_to[:t_from.size()[0], t_from.size()[-1]:]),  dim=-1)\n",
    "  to_ret = torch.cat((to_ret, t_to[t_from.size()[0]:]))\n",
    "  return to_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1686860569827,
     "user": {
      "displayName": "Fatih Erdoğan",
      "userId": "07677762697162902920"
     },
     "user_tz": -180
    },
    "id": "rORx3qcVinYS"
   },
   "outputs": [],
   "source": [
    "def merge_type3(t_from, t_to):\n",
    "  assert t_from.dim() == t_to.dim(), \"Dimensions, from-to don't hold\"\n",
    "  assert t_from.dim() == 2, f\"Expected 1D tensor, received {t_from.dim()}\"\n",
    "  to_ret = torch.cat((t_from, t_to[:, t_from.size()[-1]:]),  dim=-1)\n",
    "  return to_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1686860569827,
     "user": {
      "displayName": "Fatih Erdoğan",
      "userId": "07677762697162902920"
     },
     "user_tz": -180
    },
    "id": "rbvVKEBEenpD"
   },
   "outputs": [],
   "source": [
    "def merge_type4(e_t_from, e_t_to, o_t_from, o_t_to, old_tokenizer, new_tokenizer, merge_out):\n",
    "  assert e_t_from.dim() == e_t_to.dim(), \"Dimensions, from-to don't hold\"\n",
    "  assert e_t_from.dim() == 2, f\"Expected 1D tensor, received {e_t_from.dim()}\"\n",
    "  e_to_ret = merge_type2(e_t_from, e_t_to)\n",
    "  o_to_ret = merge_type2(o_t_from, o_t_to)\n",
    "\n",
    "\n",
    "  old_merges = list(old_tokenizer.bpe_ranks.items())\n",
    "  new_merges = list(new_tokenizer.bpe_ranks.items())\n",
    "\n",
    "  for i in range(len(old_merges), len(new_merges)):\n",
    "    t1, t2 = new_merges[i][0]\n",
    "    i1, i2 = new_tokenizer.convert_tokens_to_ids([t1, t2])\n",
    "\n",
    "    token = t1 + t2\n",
    "    id = new_tokenizer.convert_tokens_to_ids(token)\n",
    "    e_to_ret[id] = torch.mean(torch.stack([e_to_ret[i1], e_to_ret[i2]]), dim=0)\n",
    "    if merge_out:\n",
    "      o_to_ret[id] = torch.mean(torch.stack([o_to_ret[i1], o_to_ret[i2]]), dim=0)\n",
    "\n",
    "  return (e_to_ret, o_to_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1686860574119,
     "user": {
      "displayName": "Fatih Erdoğan",
      "userId": "07677762697162902920"
     },
     "user_tz": -180
    },
    "id": "DGOwAw8uKDl9"
   },
   "outputs": [],
   "source": [
    "from torch.nn.parameter import Parameter\n",
    "def prepare_embedding_and_out_layer(old_model, new_model, old_tokenizer, new_tokenizer, merge_embed, merge_out):\n",
    "  old_embedding_layer = old_model.roberta.embeddings\n",
    "  new_embedding_layer = new_model.roberta.embeddings\n",
    "  old_lm_head_layer = old_model.lm_head\n",
    "  new_lm_head_layer = new_model.lm_head\n",
    "\n",
    "  # position_embeddings E\n",
    "  new_model.roberta.embeddings.position_embeddings.weight = Parameter(merge_type3(\n",
    "      old_embedding_layer.position_embeddings.weight.data,\n",
    "      new_embedding_layer.position_embeddings.weight.data))\n",
    "  # token_type_embeddings E\n",
    "  new_model.roberta.embeddings.token_type_embeddings.weight = Parameter(merge_type3(\n",
    "      old_embedding_layer.token_type_embeddings.weight.data,\n",
    "      new_embedding_layer.token_type_embeddings.weight.data))\n",
    "  # LayerNorm E\n",
    "  new_model.roberta.embeddings.LayerNorm.weight = Parameter(merge_type1(\n",
    "      old_embedding_layer.LayerNorm.weight.data,\n",
    "      new_embedding_layer.LayerNorm.weight.data))\n",
    "\n",
    "  # dense O\n",
    "  new_model.lm_head.dense.weight = Parameter(merge_type2(\n",
    "      old_lm_head_layer.dense.weight.data,\n",
    "      new_lm_head_layer.dense.weight.data))\n",
    "  # layer_norm O\n",
    "  new_model.lm_head.layer_norm.weight = Parameter(merge_type1(\n",
    "      old_lm_head_layer.layer_norm.weight.data,\n",
    "      new_lm_head_layer.layer_norm.weight.data))\n",
    "\n",
    "  new_embed_t = None\n",
    "  new_dec_t = None\n",
    "  if merge_embed:\n",
    "    new_embed_t, new_dec_t = merge_type4(\n",
    "        old_embedding_layer.word_embeddings.weight.data,\n",
    "        new_embedding_layer.word_embeddings.weight.data,\n",
    "        old_lm_head_layer.decoder.weight.data,\n",
    "        new_lm_head_layer.decoder.weight.data,\n",
    "        old_tokenizer,\n",
    "        new_tokenizer,\n",
    "        merge_out)\n",
    "  else:\n",
    "    new_embed_t = merge_type2(old_embedding_layer.word_embeddings.weight.data,\n",
    "                                new_embedding_layer.word_embeddings.weight.data)\n",
    "    new_dec_t = merge_type2(old_lm_head_layer.decoder.weight.data,\n",
    "                              new_lm_head_layer.decoder.weight.data)\n",
    "  # word_embeddings E\n",
    "  new_model.roberta.embeddings.word_embeddings.weight = Parameter(new_embed_t)\n",
    "  # decoder O\n",
    "  new_model.lm_head.decoder.weight = Parameter(new_dec_t)\n",
    "\n",
    "  return new_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 638,
     "status": "ok",
     "timestamp": 1686860577312,
     "user": {
      "displayName": "Fatih Erdoğan",
      "userId": "07677762697162902920"
     },
     "user_tz": -180
    },
    "id": "8-xS1c_CUEm8"
   },
   "outputs": [],
   "source": [
    "def prepare_encoder_layer(old_model, new_model):\n",
    "  old_encoder_layers = old_model.roberta.encoder.layer\n",
    "  new_encoder_layers = new_model.roberta.encoder.layer\n",
    "\n",
    "  for idx, layer in enumerate(old_encoder_layers):\n",
    "    # handles a single layer\n",
    "\n",
    "    # query_a\n",
    "    new_model.roberta.encoder.layer[idx].attention.self.query.weight = Parameter(merge_type2(\n",
    "        layer.attention.self.query.weight.data,\n",
    "        new_encoder_layers[idx].attention.self.query.weight.data))\n",
    "    # key_a\n",
    "    new_model.roberta.encoder.layer[idx].attention.self.key.weight = Parameter(merge_type2(\n",
    "        layer.attention.self.key.weight.data,\n",
    "        new_encoder_layers[idx].attention.self.key.weight.data))\n",
    "    # value_a\n",
    "    new_model.roberta.encoder.layer[idx].attention.self.value.weight = Parameter(merge_type2(\n",
    "        layer.attention.self.value.weight.data,\n",
    "        new_encoder_layers[idx].attention.self.value.weight.data))\n",
    "    # dense_a\n",
    "    new_model.roberta.encoder.layer[idx].attention.output.dense.weight = Parameter(merge_type2(\n",
    "        layer.attention.output.dense.weight.data,\n",
    "        new_encoder_layers[idx].attention.output.dense.weight.data))\n",
    "    # LayerNorm_a\n",
    "    new_model.roberta.encoder.layer[idx].attention.output.LayerNorm.weight = Parameter(merge_type1(\n",
    "        layer.attention.output.LayerNorm.weight.data,\n",
    "        new_encoder_layers[idx].attention.output.LayerNorm.weight.data))\n",
    "    # dense_i\n",
    "    new_model.roberta.encoder.layer[idx].intermediate.dense.weight = Parameter(merge_type2(\n",
    "        layer.intermediate.dense.weight.data,\n",
    "        new_encoder_layers[idx].intermediate.dense.weight.data))\n",
    "    # dense_o\n",
    "    new_model.roberta.encoder.layer[idx].output.dense.weight = Parameter(merge_type2(\n",
    "        layer.output.dense.weight.data,\n",
    "        new_encoder_layers[idx].output.dense.weight.data))\n",
    "    # LayerNorm_o\n",
    "    new_model.roberta.encoder.layer[idx].output.LayerNorm.weight = Parameter(merge_type1(\n",
    "        layer.output.LayerNorm.weight.data,\n",
    "        new_encoder_layers[idx].output.LayerNorm.weight.data))\n",
    "\n",
    "  return new_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1686860578896,
     "user": {
      "displayName": "Fatih Erdoğan",
      "userId": "07677762697162902920"
     },
     "user_tz": -180
    },
    "id": "0Hk-q1_mE48P"
   },
   "outputs": [],
   "source": [
    "def prepare_model_for_new_phase(old_model, new_model, old_tokenizer, new_tokenizer, merge_embed, merge_out):\n",
    "  old_model.to(torch.device('cpu'))\n",
    "  new_model.to(torch.device('cpu'))\n",
    "  model = prepare_embedding_and_out_layer(old_model, new_model, old_tokenizer, new_tokenizer, merge_embed, merge_out)\n",
    "  model = prepare_encoder_layer(old_model, model)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 496,
     "status": "ok",
     "timestamp": 1686860579907,
     "user": {
      "displayName": "Fatih Erdoğan",
      "userId": "07677762697162902920"
     },
     "user_tz": -180
    },
    "id": "dcmsOq6Fz_C_"
   },
   "outputs": [],
   "source": [
    "def preprocess_file(file_path, tokenizer, max_length = 506):\n",
    "\n",
    "  with open(file_path, 'r') as file:\n",
    "      lines = file.readlines()\n",
    "\n",
    "  output_file_path = file_path + \"v2\"\n",
    "  output_file = open(output_file_path, 'w')\n",
    "\n",
    "  for line in lines:\n",
    "      if line.strip() == \"\":\n",
    "        continue\n",
    "      tokenized_line = tokenizer.tokenize(line)\n",
    "      if len(tokenized_line) > max_length:\n",
    "          split_num = (len(tokenized_line) // 506) + 2\n",
    "          split_line = line.split(\" \")\n",
    "          split_len = (len(split_line) // split_num)\n",
    "          sublines = [\" \".join(split_line[idx * split_len: (idx + 1) * split_len]) for idx in range(split_num + 1)]\n",
    "\n",
    "          for subline in sublines:\n",
    "              output_file.write(subline.strip() + '\\n')\n",
    "      else:\n",
    "          output_file.write(line)\n",
    "\n",
    "  output_file.close()\n",
    "\n",
    "  return file_path + \"v2\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 322,
     "status": "ok",
     "timestamp": 1686860822660,
     "user": {
      "displayName": "Fatih Erdoğan",
      "userId": "07677762697162902920"
     },
     "user_tz": -180
    },
    "id": "pvKei5IxGA2a"
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM, AdamW, get_linear_schedule_with_warmup, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset, Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def train_model(model, tokenizer, file_paths, batch_size, num_of_epochs, save_path=None, max_length=512, accumulation_steps=4):\n",
    "\n",
    "  new_file_paths = []\n",
    "\n",
    "  for file_path in file_paths:\n",
    "    new_path = preprocess_file(file_path, tokenizer)\n",
    "    new_file_paths.append(new_path)\n",
    "\n",
    "\n",
    "  device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "  model.to(device)\n",
    "  model.train()\n",
    "  eos = tokenizer.eos_token_id\n",
    "\n",
    "  def encode_examples(examples):\n",
    "    tokenized = tokenizer(examples['text'], truncation=True, padding='max_length', max_length=max_length, return_attention_mask=True)\n",
    "    labels = np.copy(tokenized['input_ids'])\n",
    "    mask = tokenized['attention_mask']\n",
    "    if labels[-1] == eos:\n",
    "      print(\"entered eos\")\n",
    "      labels[-1] = tokenizer.pad_token_id\n",
    "      labels[-2] = tokenizer.eos_token_id\n",
    "      mask[-1] = 0\n",
    "\n",
    "    labels[mask == 0] = -100\n",
    "    return {\"input_ids\": tokenized['input_ids'], \"labels\": labels, \"attention_mask\": tokenized['attention_mask']}\n",
    "\n",
    "  optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "  for epoch in range(num_of_epochs):\n",
    "    print(f\"In epoch: {epoch+1}\")\n",
    "\n",
    "    for file_path in new_file_paths:\n",
    "      print(f\"Training on file: {file_path}\")\n",
    "\n",
    "      # Load and preprocess the dataset\n",
    "      dataset = load_dataset('text', data_files=file_path)\n",
    "      tokenized_dataset = dataset.map(encode_examples)\n",
    "      tokenized_dataset.set_format(type='torch', columns=['input_ids', 'labels', 'attention_mask'])\n",
    "\n",
    "      data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "      dataloader = DataLoader(tokenized_dataset['train'], batch_size=batch_size, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "      total_steps = len(dataloader) * num_of_epochs / accumulation_steps\n",
    "      scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.1*total_steps, num_training_steps=total_steps)\n",
    "\n",
    "      progress_bar = tqdm(dataloader, position=0, leave=True)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      total_loss = 0.0\n",
    "      total_steps = 0\n",
    "\n",
    "      for idx, batch in enumerate(progress_bar):\n",
    "        inputs = {key: val.to(device) for key, val in batch.items()}\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss = loss / accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "        total_steps += 1\n",
    "\n",
    "        if (idx+1) % accumulation_steps == 0:\n",
    "          optimizer.step()\n",
    "          scheduler.step()\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "        avg_loss = total_loss / total_steps\n",
    "\n",
    "        progress_bar.set_description(f\"Loss: {avg_loss}\")\n",
    "        progress_bar.update()\n",
    "\n",
    "    if save_path is not None:\n",
    "      model.save_pretrained(save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 334,
     "status": "ok",
     "timestamp": 1686860827299,
     "user": {
      "displayName": "Fatih Erdoğan",
      "userId": "07677762697162902920"
     },
     "user_tz": -180
    },
    "id": "WU4Q6cC7oLrN"
   },
   "outputs": [],
   "source": [
    "####### File paths to be processed on phase N ########\n",
    "######################################################\n",
    "phase1_data = [\"babylm_data/babylm_10M/1-aochildes.train\", \"babylm_data/babylm_10M/2-qed.train\"]\n",
    "phase2_data = [\"babylm_data/babylm_10M/3-open_subtitles.train\", \"babylm_data/babylm_10M/4-switchboard.train\", \"babylm_data/babylm_10M/5-cbt.train\"]\n",
    "phase3_data = [\"babylm_data/babylm_10M/6-children_stories.train\", \"babylm_data/babylm_10M/7-gutenberg.train\", \"babylm_data/babylm_10M/8-simple_wikipedia.train\",\n",
    "               \"babylm_data/babylm_10M/9-wikipedia.train\", \"babylm_data/babylm_10M/99-bnc_spoken.train\"]\n",
    "all_data = phase1_data + phase2_data + phase3_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8aQSh5tzNWI"
   },
   "outputs": [],
   "source": [
    "m1, t1 = init_model_and_tokenizer(1)\n",
    "train_model(m1,t1, phase1_data, 32, 10, \"/content/drive/MyDrive/Code_BabyLM/models/modelP1\") ## Path to save the first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OeMLqX7YzPva"
   },
   "outputs": [],
   "source": [
    "m2, t2 = init_model_and_tokenizer(2)\n",
    "prepare_model_for_new_phase(m1, m2, t1, t2, True, True)\n",
    "train_model(m2, t2, phase2_data, 64, 5, \"/content/drive/MyDrive/Code_BabyLM/models/modelP2\") ## Path to save the second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xrl4W0GdzTOe"
   },
   "outputs": [],
   "source": [
    "m3, t3 = init_model_and_tokenizer(3)\n",
    "prepare_model_for_new_phase(m2, m3, t2, t3, True, True)\n",
    "train_model(m3, t3, phase3_data, 32, 3, \"/content/drive/MyDrive/Code_BabyLM/models/modelP3\") ## path to save the third model"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN28bvtlKOBlsyA5JdEmaHM",
   "gpuType": "A100",
   "mount_file_id": "1JsghSZVZQ1dxAMq0WtZYaQ_6o6vxwaKP",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
